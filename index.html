<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Evan Leong - MAE 4190 </title>
        <link rel="icon" type="image/x-icon" href="assets/img/profile_photo.jpg" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v6.3.0/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
        <!-- Mathjax CDN -->
        <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
        <!-- Jquery -->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
        <!-- Image Lightboxes-->
        <link href="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.3/css/lightbox.min.css" rel="stylesheet">
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.3/js/lightbox.min.js"></script>
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
            <a class="navbar-brand js-scroll-trigger" href="#page-top">
                <span class="d-block d-lg-none">Evan Leong</span>
                <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="assets/img/profile_photo.jpg" alt="..." /></span>
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
            <div class="collapse navbar-collapse outer " id="navbarResponsive" style = "max-height: 50vh" >
                <div class = "inner">
                    <ul class="navbar-nav">
                        <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#about">About</a></li>
                        <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#lab1a">Lab 1: Part A</a></li>
                        <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#lab1b">Lab 1: Part B</a></li>
                        <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#lab2">Lab 2: IMU </a></li>
                        <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#lab3">Lab 3: ToF </a></li>
                        <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#awards">Lab 4: Motor Drivers and Open Loop Control</a></li>
                        <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#lab5">Lab 5: Linear PID and Interpolation</a></li>
                        <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#lab6">Lab 6: Orientation PID</a></li>
                        <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#lab7">Lab 7: Kalman Filtering</a></li>
                        <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#lab8">Lab 8: Stunts!</a>
                    </ul>
                </div>
            </div>
        </nav>
        <!-- Page Content-->
        <div class="container-fluid p-0">
            <!-- About-->
            <section class="resume-section" id="about">
                <div class="resume-section-content">

                    <h1 class="mb-0">
                        MAE <span class="text-primary typewriter" id="typewriter-text"></span>
                    </h1>
                   
                    <div class="subheading mb-5">
                        Evan Leong · Ithaca, NY ·
                        <a href="mailto:epleon36@gmail.com">epleon36@gmail.com</a>
                    </div>
                    <p class="lead mb-5">Welcome! This is my project course page for MAE 4190: Fast Robots. I'm currently a senior at Cornell majoring in Computer Science with a minor in Mechanical Engineering. I enjoy rock climbing, baking, and petting dogs in my free time.</p>
                    <div class="social-icons d-flex justify-content-start align-items-center">
                        <div class = "d-flex flex-column justify-content-center">
                            <a class="social-icon" href="https://www.linkedin.com/in/evanleong1/"><i class="fab fa-linkedin-in"></i></a>
                            <p>LinkedIn</p>
                        </div>
                        <div class = "d-flex flex-column justify-content-center">
                            <a class="social-icon" href="https://github.com/evnleong"><i class="fab fa-github"></i></a>
                            <p>GitHub</p>
                        </div>
                        <div class = "d-flex flex-column align-items-center">
                            <a class="social-icon" href="https://fastrobotscornell.github.io/FastRobots-2025/"><i class="fa-solid fa-arrow-up-right-from-square"></i> </a>
                            <p>Course Website</p>
                        </div>    
                    </div>
                </div>
            </section>
            <hr class="m-0" />
            <!-- Experience-->
            <section class="resume-section" id="lab1a">
                <div class="resume-section-content">
                    <h2 class="mb-5">Lab 1A: Arduino IDE </h2>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Prelab</h3>
                            <div class="subheading mb-3">Board Library Installation</div>
                            <p> Opening the latest Arduino IDE Version 2.3.4, I installed the Sparkfun Apollo3 boards manager to install the necessary board libraries for the Artemis Nano. 
                            </p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary"></span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task 1</h3>
                            <div class="subheading mb-3">Blink Test</div>
                            <iframe src="https://drive.google.com/file/d/1YEn0QTu4YY1NJHjkGgHz0WDGWPu_YShs/preview" width="320" height="240" allow="autoplay" allowfullscreen ></iframe>
                            <p >Verified Artemis Nano functionality with Arduino's Blink sketch example.</p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary"></span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task 2</h3>
                            <div class="subheading mb-3">Serial Echo Test</div>
                            <iframe src="https://drive.google.com/file/d/1YKod5bwDx5K10vln7CG0MOWNtydJuIsH/preview" width="320" height="240" allow="autoplay" allowfullscreen></iframe>
                            <p>Verified functionality of serial communication with the Artemis Nano Board using Sparkfun's serial sketch. </p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary"></span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task 3 </h3>
                            <div class="subheading mb-3">ADC Temperature Sensor Test</div>
                            <iframe src="https://drive.google.com/file/d/1Xtx1jVAsjfpLShl3dSzs13xoxG38nBUg/preview" width="320" height="240" allow="autoplay" allowfullscreen></iframe>
                            <p>Verified functionality of the onboard ADC using Sparkfun's Temperature example sketch.</p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary"></span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task 4 </h3>
                            <div class="subheading mb-3">Microphone Test</div>
                            <iframe src="https://drive.google.com/file/d/1YjZOOAgOkHeqOGh4sgTiNilrtEc8C3i4/preview" width="320" height="240" allow="autoplay" allowfullscreen></iframe>
                            <p>Verified functionality of onboard microphone using Sparkfun's Microphone example sketch using my attempts to whistle.</p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary"></span></div>
                    </div>
                </div>
            </section>
            <hr class="m-0" />
            <!-- Lab 1B Section -->
            <section class="resume-section" id="lab1b">
                <div class="resume-section-content">
                    <h2 class="mb-5">Lab 1B: Bluetooth Setup</h2>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Prelab</h3>
                            <div class="subheading mb-3">Environment Setup</div>
                            <p> I already had Python's virtualenv package installed, so I created a new directory for Lab 1, initialized a venv named 'FastRobots_ble', and installed the required packages with pip.</p>
                            <img src="assets/img/lab1/venv.png" alt="virtual environment check" class ="img-fluid mb-3">
                            <div class="subheading mb-3">Codebase</div>
                            <p> BLE stands for Bluetooth Low Energy, a communication protocol for wireless communication between devices that consumes little power. 
                                BLE utilizes the GATT profile, which is a peripheral-central model that defines device interactions. In our codebase, after establishing a Bluetooth connection to the Nano, the functions provided through imports in the 'demo.ipynb' allow us to read, write, and subscribe to characteristics that are advertised by our Nano board. 
                                The Nano runs 'ble_arduino.ino', which contains the RobotCommand class used to parse incoming commands from the laptop client, 
                                and a switch statement (defined in handle_command()) to determine and execute the corresponding commands that we will implement throughout the lab. 
                            </p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary"></span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Configuration</h3>
                            <div class="subheading mb-3"> Generate new UUID and update Nano MAC Address </div>
                            <ul> 1. I first installed the ArduinoBLE library and then flashed the Artemis Nano with the demo sketch provided by the lab's codebase. </ul>
                            <ul> 2. The demo sketch printed out the Nano's MAC address which I made sure to left pad with 0s to get a full 12 hex address. I then updated the corresponding variable in the config .yaml file. </ul>
                            <ul> <img src="assets/img/lab1/mac_addr.png" alt="virtual environment check" class = "img-fluid mb-3"></ul>
                            <ul> 3. I then generated a UUID to replace the existing BLE (Bluetooth Low Energy Characteristic) to avoid my computer connecting to other boards and updated this in the .yaml file as well. </ul>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary"></span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task 1</h3>
                            <div class="subheading mb-3">Echo</div>
                            <script src="https://gist.github.com/evnleong/16b0096907853902ec386aace1db0272.js"></script>
                            <p> I added code to the Nano's sketch to prepend the string 'Echoing:' and postpending the ':P' emoticon to any received strings read from the TX characteristic.</p>
                            <p> Starting Jupyter Lab, I ran the relevant import statements and then called the Echo command which gave the following output.  </p>
                            <br>
                            <img src="assets/img/lab1/echo_received.png" alt = "echo received" class ="img-fluid mb-3">
                            <br>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary"></span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task 2</h3>
                            <div class="subheading mb-3">Send_Three_Floats</div>
                            <script src="https://gist.github.com/evnleong/eb33ede6cdef7e5f52797377ac2eaa30.js"></script>
                            <p>I then added the above code to loop through 3 received characters, append them to an array, and print out the 3 received floats.</p>
                            <img src = "assets/img/lab1/send_3_floats_py.png" alt = "floats sent" class = "img-fluid mb-3"> 
                            <img src = "assets/img/lab1/send_3_floats_received.png" alt = "floats received" class = "img-fluid mb-3"> 
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary"></span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task 3</h3>
                            <div class="subheading mb-3">Get_Time_Millis</div>
                            <p> I then wrote a new command called GET_TIME_MILLIS that utilizes Arduino's millis() function to get the current program time and uploaded it to the Nano. </p>
                            <script src="https://gist.github.com/evnleong/05ddb4cdddc3ac0047dfaa21e5392ffb.js"></script>
                            <p> This is the resulting output retreived by the Jupyter Notebook:</p>
                            <img src = "assets/img/lab1/get_millis_py.png" class = "img-fluid mb-3">
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary"></span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task 4 & 5</h3>
                            <div class="subheading mb-3">Notification Handler</div>
                            <p> In my Jupyter Notebook, I used the bleak package's 'start_notify' function to start notifications on the ['RX_STRING'] uuid, and created a callback function called 'notification_handler' to convert the received byte arrays into printable strings.</p>
                            <img src="assets/img/lab1/notification_handler.png" class = "img-fluid mb-3">
                            <p> I then wrote a loop function for my Arduino sketch that would continually send the current program time from the board using the millis() function for 10 seconds. </p>
                            <script src="https://gist.github.com/evnleong/8cc05399087eba1d3ea99694fe51a211.js"></script>
                            <img src = "assets/img/lab1/send_loop_received.png" class = "img-fluid mb-3">
                            <p> To calculate the effective data transfer rate, I modified my callback function to append the received messages to a Python list. I could then measure the length of the list to estimate the effective data transfer rate. </p>
                            <img src = "assets/img/lab1/effective_data_transfer.png" class = "img-fluid mb-3">
                            <p>
                                As shown above, in the 10 seconds, the Nano was able to send 253 messages (as measured by the length of the list), or 25.3 messages per second. 
                                Each transmission consisted of 1 float, or 32 bits (4 bytes) of data. The effective data transfer rate for my method was approximately:
                            </p>
                            <p>
                                \[
                                \frac{253}{10} = 25.3 \, \text{messages/second}
                                \]
                            </p>
                            <p>
                                The total data transfer rate in bytes per second is:
                            </p>
                            <p>
                                \[
                                25.3 \times 4 = 101 \, \text{bytes/second}
                                \]
                        
                            </div>
                        <div class="flex-shrink-0"><span class="text-primary"></span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task 6</h3>
                            <div class="subheading mb-3">SEND_TIME_DATA</div>
                            <p>For this task, instead of sending time measurements as soon as my board sampled them, I added the following function called 'SEND_TIME_DATA' to my Arduino sketch:</p>
                            <script src="https://gist.github.com/evnleong/107268e948a9cb514ee7161007aa9f73.js"></script>
                            <p> This function utilizes two global variables, 'timearray' and 'datasize'. 'timearray' is an empty float array initialized with size 'datasize' (in this case, my datasize was 50). 
                            The function loops up to the size of 'datasize' to guard against overfilling the array, and fills 'timearray' with 50 sequential time samplings. </p>
                            <p>It then loops through the 'timearray' and sends each reading as a formatted timestamp string.</p>
                            <img src="assets/img/lab1/time_stamps.png" class = "img-fluid mb-3"> 
                            <p>On the Jupyter Notebook, I called the 'SEND_TIME_DATA' function, and initiated the notification handler, 
                            which processed the incoming data and again appended the received data into a Python list which I could measure to ensure all 50 measurements had been sent over.</p>
                            <img src="assets/img/lab1/50_messages_received.png" class = "img-fluid mb-3"> 
                            <p> No lost measurements!</p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary"></span></div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task 7</h3>
                            <div class="subheading mb-3">GET_TEMP_READINGS</div>
                            <p>In a similar fashion, I created a new function in my Arduino sketch called GET_TEMP_READINGS:</p>
                            <p>This function samples the time and temperature each 50 times, and stored each set of 50 samples into the global 'timearray' and 'temparray' variables.</p>
                            <p>The function then loops through each array and sends the combined temperature readings to my laptop. </p>
                            <script src="https://gist.github.com/evnleong/e260e1715f8adadd2b364d74568f44d3.js"></script>
                            <p> I made a slight modification to the notification handler, prefixing "Temp Reading from Nano" to each temperature reading instead:</p>
                            <img src = "assets/img/lab1/temp_reading_handler.png" class = "img-fluid mb-3">
                            <p>On the Jupyter Notebook, I called the 'SEND_TIME_DATA' function, and initiated the notification handler to get the following results:</p>
                            <img src="assets/img/lab1/temp_readings_received.png" class = "img-fluid mb-3">

                        </div>
                        <div class="flex-shrink-0"><span class="text-primary"></span></div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task 8</h3>
                            <div class="subheading mb-3">Discussion</div>
                            <p class = "fw-bold text-secondary"> Discuss the differences between these two methods, the advantages and disadvantages of both and the potential scenarios that you might choose one method over the other.</p>
                            <p>In the first method created in tasks 4 & 5, the speed at which data could be sent was limited by the sampling rate of the millis() and temperature functions. 
                            The board physically cannot transmit the time or temp readings at a rate faster than the functions take to run.
                            However, in the second method, where time and temperature measurements are precalculated and stored in arrays, the most direct limitation of the data transfer speed seems to 
                            be the clock frequency of the Nano's CPU and however many CPU cycles it takes to append a reading from memory to the writeable GATT characteristic.</p>

                            <p>I could see how the first method has an advantage of the data being closer to real-time measurements. In a real-time feedback use case, this would likely be the preferred choice between the two methods as the readings are more closely related to the physical time that the sampling was taken.
                            However, a disadvantage of this method is the decreased speed of data transfer and increased possibility of disruptions when measurements are sent. In a case where we are okay with not receiving immediate feedback from our peripheral device, and we want to instead just need to analyze actions of the bot after they have taken place, the second method might be a more reliable and faster choice.
                            A downside to this method however is the increased RAM utilization by creating extra data structures to hold measurements before sending them to the central device.

                            <p class = "fw-bold text-secondary">How “quickly” can the second method record data?</p>
    
                            <p>
                                The time taken to send the 50 messages was calculated using the recorded time difference between the first and last sample: 64 milliseconds. 
                                Assuming this rate remains constant, the estimated message transfer speed of the Nano with this method is approximately 
                                \( \frac{50}{0.064} \approx 781 \) messages per second. 
                                Sending just one 4-byte float, this results in an effective data transfer speed of:
                            </p>
                            <p>
                                \[
                                781 \times 4 = 3,124 \text{ bytes/second}
                                \]
                            </p>
                            <p class = "fw-bold text-secondary">The Artemis board has 384 kB of RAM. Approximately how much data can you store to send without running out of memory?</p>
                            If the Artemis board has 384 kB of RAM, and we stored each value as a float, which each comprised of 4 bytes, we could theoretically store at most 96,000 total floats, or 48,000 total timestamp and temperature reading pairs before the board ran out of memory. 
                            </p>

                        </div>
                        <div class="flex-shrink-0"><span class="text-primary"></span></div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Lab 1B Discussion</h3>
                            <p>In this lab, I learned how to establish a BLE connection between my laptop and the Artemis Nano board, as well as how to create a notification handler to listen to changes on a specific GATT characteristic. </p>
                            <p>I initially struggled with implementing the notification handler and deciphering its parameters, but reading the provided lab text in the Jupyter Notebook was really helpful in understanding its implementation.</p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary"></span></div>
                    </div>
                    
                </div>
            </section>
            <hr class="m-0" />
            <!-- Lab 2-->
            <section class="resume-section" id="lab2">
                <div class="resume-section-content">
                    <h2 class="mb-5">Lab 2: IMU</h2>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Setup</h3>
                            <div class="subheading mb-3">Setting up the IMU</div>
                            <p> I first installed the 'SparkFun 9DOF IMU Breakout_ICM 20948_Arduino Library' and connected the Nano to the IMU via QWIIC connection.</p>

                            <div style="display: flex; justify-content: center;">
                                
                                <a href="assets/img/lab2/quiic.jpg" data-lightbox="gallery">
                                    <img src="assets/img/lab2/quiic.jpg" class ="zoom-effect" width="400" alt="Thumbnail">
                                </a>
                
                            </div>
                            <p style ="text-align: center" > <i>Artemis QWIIC connection to IMU </i></p>

                            <p class ="fw-bold"> Note the AD0_VAL definition. What does it represent, and should it be 0 or 1?</p>

                            <p> The AD0_VAL definition represents the value of the last bit of the IMU's I2C address. Flipping this Address 0 bit is used to change the I2C address. 
                                By default, this bit is set to 1 as the ADR jumper on my IMU is not soldered closed, but if I did solder the jumper closed, I would then need to flip the bit to 0.
                             </p> 
                            <p> I then added a visual blink indicator to the example code and uploaded it to the Nano: </p>
                            
                            <div style = "text-align: center">
                                <iframe src="https://drive.google.com/file/d/1q0yLnIzXx5e8FVou3c-n1V-HmyMUkK-3/preview" width="320" height="240" allow="autoplay" allowfullscreen></iframe>
                            </div>
                            
                            <p style = "text-align: center"> <i>Nano blinking 3 times on startup </i></p>
                            <p class = "fw-bold">Check out the change in sensor values as you rotate, flip, and accelerate the board. Explain what you see in both acceleration and gyroscope data.</p>
                            <p> The acccelerometer measures linear acceleration in the x, y, and z directions.
                                When the accelerometer is lying flat and face up in the air, the acceleration in the z-direction being recorded is approximately 9.8 m/s^2 -- corresponding to acceleration due to gravity. 
                                As I rotate the accelerometer to align with each axis, you can see as the graviational force begins to have a greater and greater component in that direction until peaking at the gravitational acceleration constant.</p>
                            <div style = "text-align: center">
                                <iframe src="https://drive.google.com/file/d/1R1nuykDzB17RRqXEVxpdAMEjkUfbmpfl/preview" width="320" height="240" allow="autoplay" allowfullscreen></iframe>
                            </div> 
                            <p> The gyroscope measures angular velocity, or the change in angle of the IMU over time. As I rotate the IMU around each axis, you can see the gyroscope values fluctuate (as the angle is changing with time) and settle at 0 when the angle of the IMU remains constant. </p>
                         </div>
                        <div class="flex-shrink-0"><span class="text-primary"></span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Accelerometer</h3>
                            <div class="subheading mb-3"> Roll and Pitch </div>
                            <p> Using the equations for deriving pitch and roll from linear accelerations in the x,y, and z directions covered in Lecture 4: </p>
                                
                            <p>
                                We can calculate roll as:
                                \[\theta = atan2(a_x, a_z)\]

                                And pitch as:
                                \[\phi = atan2(a_y, a_z)\]

                            </p>
                            <p> I added the following lines to output roll and pitch data:</p>
                           
                            <script src="https://gist.github.com/evnleong/441812b980013cce4787fbe847921872.js"></script>


                            <p> I then tested the formulas at -90, 0, 90 degree rotations for both pitch and roll to see if the output made sense: </p>

                            <div style = "text-align :center">
                                <img src="assets/img/lab2/pitchroll0.png " height = "400" width = "400" class = "img-fluid mb-3">
                            
                            <p> <i> Pitch and Roll at 0 deg. </i></p>
                            <img src="assets/img/lab2/pitch90roll0.png " height = "400" width = "400" class = "img-fluid mb-3">
                            <p> <i> Pitch at 90 deg. and Roll at 0 deg. </i></p>
                            <img src="assets/img/lab2/pitch-90roll0.png " height = "400" width = "400" class = "img-fluid mb-3">
                            <p> <i> Pitch at -90 deg. and Roll at 0 deg. </i></p>
                            <img src="assets/img/lab2/roll90pitch0.png " height = "400" width = "400" class = "img-fluid mb-3">
                            <p> <i> Pitch at 0 deg. and Roll at 90 deg. </i></p>
                            <img src="assets/img/lab2/roll-90pitch0.png " height = "400" width = "400" class = "img-fluid mb-3">
                            <p> <i> Pitch at 0 deg and Roll at -90 deg. </i></p>

                            </div>
                           
                            <div class="subheading mb-3"> Recording Data </div>
                            <ul> 1. To record data, I created a 2 dimensional array in Arduino to store lists of Pitch, Roll, and Timestamp Data.</ul>
                            <ul> 2. Dynamically allocating memory onto the heap, I was able to store 2048 data points, or approximately 5 seconds worth of timestamp data for both pitch and roll.</ul>
                            <ul> 3. Using my function 'Send_RP_Data_Store', I sent my bulk data from my Nano to my laptop via the BLE setup from Lab 1.</ul>
                            <div class="subheading mb-3"> Frequency Analysis </div>
                            <ul> 1. Before creating my PSD, I first installed the matplotlib and scipy packages.</ul>
                            <ul> 2. I then used my 'SEND_RP_DATA_STORE' to retreive 5 seconds of samples from my Nano for both Pitch and Roll and plotted the time domain signal for both pitch and roll:</ul>
                            <div class ="d-flex flex-md-row">
                                <img src ="assets/img/lab2/time_pitch_static.png" class = "img-fluid mb-3" style ="width : 50%" >  
                                <img src ="assets/img/lab2/time_roll_static.png" class = "img-fluid mb-3"style ="width : 50%" >  
                            </div>
                            
                            <ul> 3. I then introduced some noise in my data by shaking the IMU.</ul>
                            <div class ="d-flex flex-md-row">
                                <img src ="assets/img/lab2/time_pitch_motion.png" class = "img-fluid mb-3" style ="width : 50%" >  
                                <img src ="assets/img/lab2/time_roll_motion.png" class = "img-fluid mb-3"style ="width : 50%" >  
                            </div>
                            <ul> 4. I then performed the FFT to get the following PSD for each measure:</ul>
                            <div class ="d-flex flex-md-row">
                                <img src ="assets/img/lab2/fft_pitch_static.png" class = "img-fluid mb-3" style ="width : 50%" >  
                                <img src ="assets/img/lab2/fft_roll_static.png" class = "img-fluid mb-3"style ="width : 50%" >  
                            </div>
                            <div class ="d-flex flex-md-row">
                                <img src ="assets/img/lab2/fft_pitch_motion.png" class = "img-fluid mb-3" style ="width : 50%" >  
                                <img src ="assets/img/lab2/fft_roll_motion.png" class = "img-fluid mb-3"style ="width : 50%" >  
                            </div>

                            <p class="fw-bold"> Discuss how the choice of cut-off frequency affects the output.</p>
                            
                            <ul> From the results of the FFT, I selected 5 Hz as my cutoff frequency as frequencies above that value appeared to be correlated with high-frequency noise as evidenced in my plots above. </ul>
                            <ul> My choice of 5 Hz as the cut-off frequency strikes a balance between attenuating unwanted high-frequency noise (such as vibrations or random fluctuations) while preserving the important low-frequency signal (like the gradual changes in pitch and roll)
                                 that I am trying to track. This ensures that the signal remains clear and accurately represents the desired data without unnecessary distortions from high-frequency noise.</ul>

                            <div class="subheading mb-3"> Implementing Low Pass Filter </div>
                    
                            <p> From Lecture 4, we are given the following recursive formula for a low pass filter: </p>

                            <div style = "text-align: center;">
                                <p> \(\theta_{\text{LPF}}[n] = \alpha \cdot \theta_{\text{RAW}} + (1 - \alpha) \cdot \theta_{\text{LPF}}[n-1]\) </p>

                                <p> \(\theta_{\text{LPF}}[n-1] = \theta_{\text{LPF}}[n]\) </p>
                            </div>                 
                            
                            <p> However, we need to calculate a value for alpha using our selected cutoff frequency of 5Hz.</p>

                            <p> We can use the following equation for a RC low pass filter: </p>
                            <div style = "text-align: center;">
                                <p> \( f_c = \frac{1}{2\pi RC}\)</p>
                            </div>
                            <p> Where \( f_c \) is our cutoff frequency, to then solve for RC in the following equation: </p>
                            <div style = "text-align: center;">
                            <p> \(\alpha = \frac{T}{T + RC} \) </p>
                            </div>
                            <p> To which we can finally solve for \(\alpha\).</p>
                            <p> Substituting a cutoff frequency of 5 Hz in for \( f_c \), we obtain a value of 0.318 seconds for the RC time constant.  </p>
                            <p> When sending data over to my laptop, I calculated my sampling frequency to be about 400 samples/s or a sampling period of approx. 0.0025 seconds. </p>
                            <p> Substituting these two values into our last equation, we obtain an \(\alpha\) of approximately 0.078.</p>

                            <p> Below is a code snippet from my LPF implementation:</p>
                            <script src="https://gist.github.com/evnleong/8c76b7b8625c2f00fc0af4220aaad986.js"></script>

                            <p> And these are the results of the LPF on both Pitch and Roll data from the accelerometer overlaid over the raw data:</p>
                            <div class ="d-flex flex-md-row">
                                <img src ="assets/img/lab2/lpf_pitch.png" class = "img-fluid mb-3" style ="width : 50%" >  
                                <img src ="assets/img/lab2/lpf_roll.png" class = "img-fluid mb-3"style ="width : 50%" >  
                            </div>
                            <p> As shown above, a lot of the high frequency noise was able to be attenuated. </p>
    
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary"></span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Gyroscope</h3>
                            <div class="subheading mb-3">Pitch Roll Yaw</div>
                            <p> Using the following equations to calculate Pitch, Roll and Yaw from angular velocity of the gyroscope:</p>

                            <div style = "text-align: center;">
                                <p> \(\theta_g = \theta_g + \text{gyro reading} \cdot dt\) </p>
                                <p> \(\phi_g = \phi_g + \text{gyro reading} \cdot dt\) </p>
                                <p> \(\psi_g = \psi_g + \text{gyro reading} \cdot dt\) </p>
                            </div>
                            
                            <p> I added code to my Nano sketch to compute and store the output of these formulas (a relevant code snippet is shown below): </p>

                            <script src="https://gist.github.com/evnleong/9c863d474d95458d3a3646455a5fa8ea.js"></script>

                            <p>I chose to plot the output of the gyroscope as I rotated it between 0, 90, and -90 deg. angles and overlay it with both the raw and filtered pitch and roll values from the accelerometer. 
                            (Since gravity does not have a component along the yaw axis, I could not directly compare the gyroscope's yaw data with the accelerometer's
                             measurements, as the accelerometer only detects linear acceleration and cannot measure rotational motion around the yaw axis.)
                            </p>
                            <div class = "d-flex flex-md-row">
                                <img src="assets/img/lab2/pitch_gyro_accel.png" style = "width:50%" class = "img-fluid mb-3">
                                <img src="assets/img/lab2/roll_gyro_accel.png" style = "width:50%" class = "img-fluid mb-3">
                            </div>
                        
                            <p> Although the time period of data collection was brief, as can be seen in the plot above, I noticed that compared to the accelerometer, 
                                the gyroscope tends to slightly drift over time after rotations, even when reverting back to its original angle. This drifting appears to be okay during the short window shown above, but over long time periods this drift accumulates.
                                Additionally, compared to the raw accelerometer data, the raw gyroscope data had far less noise, seemingly less than even my filtered accelerometer data. 
                            </p>
                            <br>

                            <p> Lastly to test my yaw formula, I attempted to oscillate the IMU in the yaw axis between -90 deg , back to 0 deg , and 90 deg within the 5 second sampling window: </p>

                            <div style = "text-align: center;">
                                <img src="assets/img/lab2/yaw.png" style = "width:50%" class = "img-fluid mb-3">
                            </div>
                            <br>
                            <div class="subheading mb-3">Complementary Filter</div>
                           
                            <p> Using the following equation from Lecture 4: </p>
                            <div style = "text-align: center;">
                                <p> \(\theta = (\theta + \theta_g) (1 - \alpha) + \theta_a \alpha \)</p>
                            </div>
                            
                            <p> I applied a complementary filter to my IMU data to compute pitch and roll more accurately with higher stability with the following code: </p>

                            <script src="https://gist.github.com/evnleong/6b0d77e912554c613b6ac38b38dfb89d.js"></script>
                           
                            <p> I selected the value for comp_alpha to be 0.3 as I trusted the accelerometer's data more than the gyroscope, but still wanted to strike a balance between the high sensitivity and noise of the accelerometer and the drifting of the gyroscope.</p>
                            

                            <p class = "fw-bold">  Demonstrate its working range and accuracy, and that it is not susceptible to drift or quick vibrations.</p>
                            <p> To demonstrate the effectiveness of the complementary filter, I placed the IMU on a table at rest, and shook it at two intervals during the sampling period.
                                I then plotted a graph for each measurement.</p>
                            <p> As shown below, the complementary filter tries to strike a balance between the sensitive accelerometer and the drifiting gyroscope. I did notice that the complementary filter output does still drift slightly due to the contributions from the gyroscope, but at the same time it has reduced a lot of the high frequency noise from the shaking table,
                                 while still remaining responsive to subtle changes in pitch and roll that would have been missed by just the gyroscope alone.</p>
                            <div class = "d-flex flex-md-row">
                                <img src="assets/img/lab2/comp_filter_pitch.png" style = "width:50%" class = "img-fluid mb-3">
                                <img src="assets/img/lab2/comp_filter_roll.png" style = "width:50%" class = "img-fluid mb-3">
                            </div>
                            <br>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary"></span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Sample Data</h3>
                            <div class="subheading mb-3"> Sampling Data Discussion </div>
                            <p class = "fw-bold"> How quickly are you able to sample new values?</p>
                            <p> Removing any extraneous print statements and delays, I was able to sample 2048 samples in approximately 4996 milliseconds. The equivalent sample rate was approximately 400 samples per second.  </p>
                            <p class = "fw-bold"> Does your main loop on the Artemis run faster than the IMU produces new values?</p>
                            <p> My main loop on the Artemis does run faster than the IMU produces new values. In my implementation, the IMU cannot retrieve a sample unless the call to myICM.dataReady() returns true, whereas the main loop can continue to execute regardless of the return value of the function. </p>

                            <p class = "fw-bold"> Consider if it makes sense to have one big array, or separate arrays for storing accelerometer and gyroscope data, argue for your choice. </p>
                            <p> In my implementation, I chose to use multiple, separate arrays in order to store my gyroscope and accelerometer data. I felt that this made the most logical sense to me, 
                                and allowed me to easily differentiate between the different data sources when transmitting my data over BLE. </p>
                            
                            <p class = "fw-bold"> Consider the best data type to store your data. Should you use string, floats, double, integers? Justify your decision. </p>
                            <p>The primary data type I chose to store all of my data as was float. This is because the accelerometer and gyroscope sensor data required precision with decimal values and floats provide the necessary precision while being memory efficient (compared to double).
                                However, if I were to optimize further, I might consider switching from float to uint32_t for storing time values (such as those obtained from millis()), as millis() returns an integer value. Using uint32_t would likely be more appropriate for storing time because it's a positive integer value and avoids the unnecessary overhead of floating-point representation.
                                That said, I decided to stick with floats throughout for consistency across all data types. Additionally, since the memory space for storing uint32_t and float is identical, I didn't consider the trade-off significant enough to change the data type. </p>
                            <p class = "fw-bold">  Consider the memory of the Artemis; how much memory can you allocate to your arrays? What does that correspond to in seconds?</p>
                            <p> Assuming that the Artemis has the full 384 kB of RAM available to allocate to the arrays, and assuming that each sample stores  4 float data points (Roll, Pitch, Yaw, and Timestamp), this would result in:</p>

                            <p>
                                \[
                                \text{Data per sample} = 4 \times 4 \, \text{bytes} = 16 \, \text{bytes}
                                \]
                            </p>

                            <p>
                                With a total available RAM of 384 kB, we can calculate the maximum number of samples that can be stored:
                            </p>

                            <p>
                                \[
                                \text{Max samples} = \frac{384 \, \text{KB}}{16 \, \text{bytes/sample}} = \frac{384,000 \, \text{bytes}}{16 \, \text{bytes/sample}} = 24,000 \, \text{samples}
                                \]
                            </p>

                            <p>
                                So, at most 24,000 samples can be stored in total.
                            </p>

                            <p>
                                Assuming 400 samples can be collected per second, the time it would take to use up the available memory is:
                            </p>

                            <p>
                                \[
                                \text{Time before running out of memory} = \frac{24,000 \, \text{samples}}{400 \, \text{samples/sec}} = 60 \, \text{seconds}
                                \]
                            </p>

                            <p>
                                So, under these assumptions, you would be able to sample for approximately 1 minute before the available memory is exhausted.
                            </p>

                            <div class="subheading mb-3"> Store & Send 5 seconds of sample data </div>
                            
                            <p class = "fw-bold">5 seconds of time stamped data from IMU: </p>

                            
                            <div class = "d-flex flex-md-row">
                                <img src="assets/img/lab2/first_5.png" style = "width:50%" class = "img-fluid mb-3">
                                <img src="assets/img/lab2/last_5.png" style = "width:50%" class = "img-fluid mb-3">
                            </div>

                            <p> Shown above are the first and last 5 IMU sensor readings I collected from a total of 2048 readings from the Nano.
                                 From first timestamp to last timestamp, the collected data spans 5199 milliseconds (5.2 seconds) </p>

                            <p class = "fw-bold">5 seconds worth of sample data collected from IMU.</p>
                            <p> For my >5 seconds of IMU sample data, I decided to chart the raw accelerometer, raw gyroscope, and complimentary filter pitch readings over the span of those 5.2 seconds. </p>
                            <div class = "d-flex flex-md-row">
                                <img src="assets/img/lab2/5s.png" class = "img-fluid mb-3">
                            </div>
                           
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary"></span></div>
                    </div>
                    
                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Record A Stunt</h3>

                            <iframe src="https://drive.google.com/file/d/1yRxE9Nt33gGqggi2tr9niX9lRB5YDhM1/preview" width="320" height="240" allow="autoplay" allowfullscreen></iframe>

                            <p class = "fw-bold">
                               Show what you have tried, and discuss what you observe.
                            </p>

                            <p> In the stunt video I attempted a vertical flip and some targeted driving. </p>
                            <p> I observed that the car accelerates extremely quickly and is able to spin about its central axis very easily. </p>
        
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary"></span></div>
                    </div>     
                </div>
            </section>
            <hr class="m-0" />
            <!-- Interests-->
            <section class="resume-section" id="lab3">
                <div class="resume-section-content">
                    <h2 class="mb-5">Lab 3: ToF</h2>

                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Prelab</h3>

                            <p class = "fw-bold">Note the I2C sensor address.</p>
                            <p> Following the Polulu ToF sensor datasheet, the default I2C address of the ToF sensors should be 0x52. </p>
                            <p class = "fw-bold"> Briefly discuss the approach to using 2 ToF sensors.</p>
                            <p> The approach to using 2 ToF sensors is to enhance the robot's overall depth perception and ability to measure the distance from my car to any obstacles by fusing data from multiple ToF sensors. </p>
                            <p class = "fw-bold"> Briefly discuss placement of sensors on robot and scenarios where you will miss obstacles.</p>
                            <p> In my approach, I plan on putting 1 ToF sensor at the front of the car and 1 ToF sensor at the rear. My reasoning for this is that placing both on the front will likely lead to an overlap in the FOVs of the sensors, and because I am limited in the number of ToF sensors I have,
                                I want to maximize the coverage that my ToF sensors provide. Thus, by placing a sensor at the rear I can observe obstacles in the rear of my vehicle without requiring an entire 360 degree rotation. A potential option could have been to place a ToF sensor on the side of the vehicle to detect the obstacles that I would miss on either side of the robot, but as I am constrained by only having 2 ToF, 
                                I would have to make a sacrifice in that I would only be able to detect obstacles on 1 side of the robot at a time without rotating, which I reason would be just as useful if 
                                I were to just rotate my robot 90 deg and use either the rear or forward ToF sensors to detect the obstacle. Thus with this setup I will miss detection in scenarios where obstacles or walls appear to the sides of my robot, and that are outside the FOVs of the front and rear ToF sensors.</p>
                            <p class = "fw-bold"> Sketch of wiring diagram (with brief explanation if you want) </p>
                            <img src= "assets/img/lab3/tofschematic.png" class = "img-fluid mb-3"> 

                            <p>Above is a sketch of my initial wiring of the 2 ToF sensors to the Nano using the Qwiic multiport connector. I will be using the 2 longest Qwiic cables for the ToF as they will need to be placed on opposite ends of the robot.</p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary"></span></div>
                    </div>     


                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task 1</h3>
                            <div class="subheading mb-3">Powering Up Nano Off Battery</div>

                            <div style = "text-align: center;">
                                <iframe src="https://drive.google.com/file/d/1kbTRgsJACsL1_UE7l8BIY3lxAa1z7klO/preview" width="320" height="240" allow="autoplay" allowfullscreen></iframe>
                                <p> <i> Video of Nano powering up off battery (running Lab 2 sketch).</i>   </p>
                            </div>                       
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary"></span></div>
                    </div>     

                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task 2</h3>

                            <div class="subheading mb-3">ToF sensor connection</div>

                            <p class = "fw-bold"> Picture of your ToF sensor connected to your QWIIC breakout board.</p>

                            <div style = "text-align: center;">
                                <img src = "assets/img/lab3/tofconnection.jpg" width = 240 height = 240 class = "img-fluid mb-3"> 
                                
                            </div>

                            <p class = "fw-bold"> Screenshot of Artemis scanning for I2C device (and discussion on I2C address). Does the address match what you expected? If not, explain why. </p> 
                            <div style = "text-align: center;">
                                <img src= "assets/img/lab3/tofaddress.png" class = "img-fluid mb-3">
                            </div>

                            <p> The address returned by the I2C scanner code did not match my expectations. Following the Polulu datasheet, we would expect the scanner to detect the ToF sensor at its stated default address of 0x52, but instead it returns 0x29.
                                 After some research, it appears this is because the 0x52 value stated on the datasheet is the full I2C address range, where the first 7 bits of the range (1010010) is the actual slave address, and the last bit in the range signifies data direction (read or write). 
                                 Just taking the 7 bit address of 0101001 and ignoring the read/write bit, we get the I2C address returned by the scanner code which is 0x29.</p>
                            <div class="subheading mb-3">ToF Setup</div>

                            <p class = "fw-bold"> The ToF sensor has three modes, that optimize the ranging performance given the maximum expected range. 
                                Discuss the pros/cons of each mode, and think about which one could work on the final robot. <pre> .setDistanceModeShort(); //1.3m .setDistanceModeMedium(); //3m .setDistanceModeLong(); //4m, Default </pre></p>
                            
                            <img src = "assets/img/lab3/distancemodes.png" class= "img-fluid mb-3"> 
                            <p> According to the datasheet (a relevant snippet shown above), setDistanceModeShort() appears to be the most resilient to ambient light, but its drawback is that it is the most limited in sensing range to approximately 1.35m. </p>
                            
                            <p> setDistanceModeLong() appears to be easily impacted by ambient light, but has the greatest sensing range of 4 meters.</p>

                            <p> setDistanceModeMedium() seems to be a good balance between both the short and long distance modes, but this also means it is not the best of the options in either metric. Also, looking at the datasheet, it appears that it's maximum sensing range is also greatly impacted by ambient light.</p>

                            <p> After deliberating on which sensor mode to pick, I settled on using the short distance mode. I expect ambient light interference to be a prevalent issue that will likely be out of my control throughout the labs, and by choosing the short distance mode, I provide the greatest resilience to this interference that is within my control.
                                Furthermore, after reviewing the datasheet, the max ranging distance of the medium and long distance modes appear to be so significantly hindered by ambient light that the short distance mode actually has a higher maximum ranging distance in strong ambient light conditions. I believe a consistent 1.35m of ranging distance to be more than enough for detection of obstacles to the front and rear of my robot in a small room situation and more ideal than highly variable, albeit further sensing distances, provided by the other modes. </p>
                           
                            <div class="subheading mb-3">Test Chosen ToF distance Mode</div>

                            <p> At this point in the lab, I installed the SparkFun VL53L1X 4m laser distance sensor library.</p>

                            <p> I then ran the demo code, and tested the sensor on my lab box placed approximately 90 mm away. </p>
                            
                            <div style = "text-align:center;">
                                <img src="assets/img/lab3/tofexample.png" class ="img-fluid mb-3"> 
                            </div>
                            
                            
                            <div class="subheading mb-3">ToF in Parallel</div>
                            <p class = "fw-bold"> Using notes from the pre-lab, hook up both ToF sensors simultaneously and demonstrate that both work.</p>
                            
                            <p> At this point in the lab, I soldered a connection between the XSHUT pin on one ToF sensor and the A2 pin on the Nano. Since both ToF sensors have the same default I2C address, adding this jumper allows me to disable one of the ToF sensors using the XSHUT pin to allow
                                me to address the active sensor and reassign it to a unique I2C address (in this case 0x4A). I can then reboot the powered off ToF sensor, and now properly address both sensors in parallel. </p>


                            <p> Below is my updated schematic: </p>

                            <img src = "assets/img/lab3/tofschematicupdate.png" class = "img-fluid mb-3"> 

                            <p> And a snapshot of my new wiring: </p>
                            
                            <div style = "text-align: center;">
                                <img src = "assets/img/lab3/tofconnectionupdate.jpg" width = 300 height = 300 class = "img-fluid mb-3"> 
                            </div>
                            
                            <p class = "fw-bold">
                                2 ToF sensors and the IMU: Discussion and screenshot/video of sensors working in parallel
                            </p>

                            <p> Using the following code implementing the shut off routine outlined earlier, I was able to get the two ToF sensors to print out measurement data in parallel whenever data was ready on both sensors:</p>
                            <script src="https://gist.github.com/evnleong/62128dc1bd3a330f3439c061e25740eb.js"></script>
                            <img src = "assets/img/lab3/tofparallel.png" class = "img-fluid mb-3"> 

                            <div class="subheading mb-3">Documenting ToF Accuracy</div>
                            <p> To test the accuracy of the sensors, I attached the 2 ToF sensors to a cardboard box directly across from my lab box, and moved the sensors back in increasing intervals of 50mm along a ruler,
                                plotting the actual distance to the target vs a sample from each sensor's reported distance. An image of my setup is shown below (In later iterations I switched to using a measuring tape as my iPad wasn't long enough and I was worried about possible ambient light interference).
                           </p>

                           <div style = "text-align:center;">

                            <img src = "assets/img/lab3/tofsetup.jpg" height = 300 width = 300 class = "img-fluid mb-3">

                           </div>

                           <p> To maximize the accuracy and precision of my sensor, I tried to reduce ambient light in the room as much as possible by turning off most lights (more ambient light would decrease ranging distance), and also chose my white lab box (as opposed to a darker surface) as it would hopefully reflect more of the IR light emitted by the ToF back to the sensor.</p>

                           <img src = "assets/img/lab3/tofaccuracy.png" class = "img-fluid mb-3">

                           <p> In this test, I noticed that at distance 0, even though my ToF sensors were directly touching the target box, one of my ToF sensors still detected the target at a nonzero distance (after multiple trials, this error was usually 10+ mm). 
                               This same sensor also consistently underreported distances to the target by approximately 15-30 mm at each measurement point which I thought was odd. 
                               Further, I noticed that for the first 150 mm or so, the ToF sensor data for both sensors was much further off from the expected distance to the target, which is a little higher than the datasheet's stated minimum ranging distance of around 4 cm.  
                           </p>

                           <div class="subheading mb-3">Documenting Repeatability</div>
                           <p> To test the repeatability of the sensors, I conducted the same test but instead of taking just 1 sample at each point, I took 10 samples at each measurement point, and calculated the std. deviation between the samples taken at each point. Below is the plot of those std deviations.</p>
                           
                           <img src= "assets/img/lab3/tofrepeatability.png" class = "img-fluid mb-3">
                           
                           <p> The std. deviation appears to fluctate randomly (except on sensor 2 when pushed against the box (it consistently read 0)) across all data points, but overall the std. deviation amongst datapoints for a measurement tended to be around 1-2 mm.</p>

                           <div class="subheading mb-3">Documenting Varied ToF Ranging Times</div>
                            <p> I also tested modifying the timing budget parameter for one of my sensors using the setTimingBudgetInMs() function.  </p>
                            <p> According to the datasheet, "increasing the timing budget increases the maximum distance the device can range and
                                improves the repeatability error. However, average power consumption augments accordingly." Thus, I changed my testing
                                setup to measure a max range of 1500 to test how varying timing budgets would affect the maximum ranging distance and repeatability of my sensor. 
                            </p>

                            <img src = "assets/img/lab3/tofbudgetaccuracy.png" class ="img-fluid mb-3">

                            <p> Interesting to note from this test was that with the timing budget set to 20 ms, even after multiple attempts, the ToF failed to detect the obstacle from ranges 1200 mm and beyond with dramatic fluctations in measurements.
                                With timing budgets of 50mm and 100mm however, the ToF sensor appeared to be able to detect the lab box at these far distances but was still relatively inaccurate, which aligns with my expectations from the datasheet that the ToF sensors have a max range (in short distance mode) of around 1.35-1.4m. </p>
                            
                            <p> Then following the similar setup as before, I again took 10 measurements at each point with each timing budget and I plotted the std deviation of the obtained datapoints at each measured distance.</p>
                            <img src = "assets/img/lab3/tofbudgetrepeatability.png" class ="img-fluid mb-3">

                            <p> As shown above, I noticed that generally as the ranging distance increased, standard deviation increased as well, but that by increasing the timing budget, there was a fair decrease in repeatability error, which aligns with the statement 
                            from the datasheet that higher timing budgets decrease repeatability error.</p>
                            
                            <p> One thing to note was that with a timing budget of 20ms, a huge spike in the std. deviation was observed at the 1500 mm mark. As mentioned earlier,even after repeated trials it did not seem like I could get the ToF sensor to detect the lab box with a timing budget of 20ms, 
                                and readings at this distance could likely be attributed to slight movement in the angle of sensors shifting in their tape over time causing them to detect other objects or some other form of noise in the system.</p>
                           
                            <p> Lastly, I was also able to send timestamped ToF data from my sensors in parallel to my laptop over BLE and plotted the readings below.</p>
                            <img src = "assets/img/lab3/toftimestampchart.png" class = "img-fluid mb-3"> 
                    
                            <p class ="fw-bold">  Write a piece of code that prints the Artemis clock to the Serial as fast as possible, continuously, and prints new ToF sensor data from both sensors only when available.
                                The distanceSensor.checkForDataReady() routine can be called to check when new data is available.
                                How fast does your loop execute, and what is the current limiting factor?</p>

                                <p>Using the following code, I printed the Artemis clock to the Serial port as fast as possible, only displaying readings from the ToF sensors when new data was available.</p>
                                <script src="https://gist.github.com/evnleong/c7fb19ad2fbdcc402ef657279af0ab52.js"></script>
                                <p> Using the timestamp values generated by the millis() function call (a snippet is shown below), the average time between millis() readings was approximately 13 ms, which gives us an execution rate of the outer loop somewhere around 77 Hz. </p>
                                <div style = "text-align: center;">
                                    <img src ="assets/img/lab3/clocktest.png" class = "img-fluid mb-3" >  
                                </div>
                                <p> The average time between actual ToF measurements for each individual sensor was around 150 millis, or approximately 6 Hz. </p>
                                <p> Thus, it appears the current limiting factor are the calls to the checkForDataReady() functions. </p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary"></span></div>
                    </div>     

                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task 3 </h3>
                            <div class="subheading mb-3">ToF Data Collection</div>

                            <p>Lastly, I reconnected the IMU and updated my IMU retrieval function from Lab 2 to include ToF data as well. (A relevant code snippet shown below) </p>
                            
                            <script src="https://gist.github.com/evnleong/28b170b2a562804113731f37f0a06158.js"></script>
            
                            <p>I then sent timestamped data of roll, pitch, yaw, and distance from both ToF sensors all at once to my laptop.  </p>

                            <p class = "fw-bold"> Include a plot of the ToF data against time. (Time vs Distance) </p>
                            
                            <img src="assets/img/lab3/tofwimu.png" class = "img-fluid mb-3"> 
                        
                            <p class = "fw-bold"> Include a plot of the IMU data against time. (Time vs Angle) </p>

                            <img src="assets/img/lab3/imuwtof.png" class = "img-fluid mb-3"> 

                        </div>
                        <div class="flex-shrink-0"><span class="text-primary"></span></div>
                    </div>   

                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Collaboration Statement </h3>
                            

                            <p>I referenced Nila Narayan's Github page for advice on documenting sensor accuracy, repeatability, and varied ranging times! <a href = "https://nila-n.github.io/Lab3.html"> https://nila-n.github.io/Lab3.html </a> </p>
                        

                        </div>
                        <div class="flex-shrink-0"><span class="text-primary"></span></div>
                    </div>   
                    
                </div>
            </section>
            <hr class="m-0" />
            <!-- Awards-->
            <section class="resume-section" id="awards">
                <div class="resume-section-content">
                    <h2 class="mb-5">Awards & Certifications</h2>
                    <ul class="fa-ul mb-0">
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            Google Analytics Certified Developer
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            Mobile Web Specialist - Google Certification
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            1
                            <sup>st</sup>
                            Place - University of Colorado Boulder - Emerging Tech Competition 2009
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            1
                            <sup>st</sup>
                            Place - University of Colorado Boulder - Adobe Creative Jam 2008 (UI Design Category)
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            2
                            <sup>nd</sup>
                            Place - University of Colorado Boulder - Emerging Tech Competition 2008
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            1
                            <sup>st</sup>
                            Place - James Buchanan High School - Hackathon 2006
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            3
                            <sup>rd</sup>
                            Place - James Buchanan High School - Hackathon 2005
                        </li>
                    </ul>
                </div>
            </section>

            <hr class="m-0" />

            <!-- Lab 5 Section -->
            <section class="resume-section" id="lab5">
                <div class="resume-section-content">
                    <h2 class="mb-5">Lab 5: Linear PID and Interpolation </h2>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Prelab</h3>
                            <div class="subheading mb-3">Environment Setup</div>
                            <p> I already had Python's virtualenv package installed, so I created a new directory for Lab 1, initialized a venv named 'FastRobots_ble', and installed the required packages with pip.</p>
                            
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary"></span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Configuration</h3>
                            <div class="subheading mb-3"> Generate new UUID and update Nano MAC Address </div>
                            <ul> 1. I first installed the ArduinoBLE library and then flashed the Artemis Nano with the demo sketch provided by the lab's codebase. </ul>
                            <ul> 2. The demo sketch printed out the Nano's MAC address which I made sure to left pad with 0s to get a full 12 hex address. I then updated the corresponding variable in the config .yaml file. </ul>

                            <ul> 3. I then generated a UUID to replace the existing BLE (Bluetooth Low Energy Characteristic) to avoid my computer connecting to other boards and updated this in the .yaml file as well. </ul>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary"></span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task 1</h3>
                            <div class="subheading mb-3">Echo</div>
                            <script src="https://gist.github.com/evnleong/16b0096907853902ec386aace1db0272.js"></script>
                            <p> I added code to the Nano's sketch to prepend the string 'Echoing:' and postpending the ':P' emoticon to any received strings read from the TX characteristic.</p>
                            <p> Starting Jupyter Lab, I ran the relevant import statements and then called the Echo command which gave the following output.  </p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary"></span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task 2</h3>
                            <div class="subheading mb-3">Send_Three_Floats</div>
                            <script src="https://gist.github.com/evnleong/eb33ede6cdef7e5f52797377ac2eaa30.js"></script>
                            <p>I then added the above code to loop through 3 received characters, and appending them to an array, printing out the 3 received floats.</p>

                        </div>
                        <div class="flex-shrink-0"><span class="text-primary"></span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task 3</h3>
                            <div class="subheading mb-3">Get_Time_Millis</div>
                            <p> I then wrote </p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary"></span></div>
                    </div>
                </div>
            </section>
        </div>

        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>


